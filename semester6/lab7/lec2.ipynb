{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "hsbyb4tyki9nx32utdtjpk",
    "id": "71AQJg3CDMn9"
   },
   "source": [
    "# Deep learning для классификации картинок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семинар состоит из 2 частей\n",
    "\n",
    "1. Ознакомьтесь со структурой обычного обучающего скрипта и потренируйте старую добрую сеть, подобную vgg\n",
    "2. Улучшите качество с помощью сети, подобной resnet\n",
    "\n",
    "Но сначала посмотрим на данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vgg-neural-network-architecture.png\" alt=\"Drawing\" style=\"width:90%\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "34l2kkk7t84llsmzyxus1",
    "id": "2MaELIpIDMoA"
   },
   "source": [
    "# Tiny ImageNet dataset\n",
    "На этом семинаре мы сосредоточимся на задаче распознавания изображений на Tiny Image Net dataset. Этот набор данных содержит\n",
    "* 100 тысяч изображений размером 3x64x64\n",
    "* 200 различных классов: змеи, пауки, кошки, грузовики, кузнечики, чайки и т.д.\n",
    "\n",
    "На самом деле, это подмножество набора данных ImageNet с изображениями, уменьшенными в 4 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "rjwf5t0s4f8zbnfmyu7q",
    "id": "swKtJaVyDMoU"
   },
   "source": [
    "## Image examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "nbxuu26h8hhcgzzgeh5nh",
    "id": "h5wImXEaDMoV"
   },
   "source": [
    "\n",
    "\n",
    "<tr>\n",
    "    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tinyim3.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n",
    "    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tinyim2.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n",
    "</tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "w71ngep3y8jm2s3xt0qg",
    "id": "Do-qRQp8DMoW"
   },
   "source": [
    "<tr>\n",
    "    <td> <img src=\"https://github.com/yandexdataschool/Practical_DL/blob/sem3spring2019/week03_convnets/tiniim.png?raw=1\" alt=\"Drawing\" style=\"width:90%\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "5nh892g5zpl9qv5fki8vpk",
    "id": "5rQhiYyRDMoG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was downloaded to '.\\tiny-imagenet-200.zip'\n",
      "Extract downloaded dataset to '.'\n"
     ]
    }
   ],
   "source": [
    "from tiny_img import download_tinyImg200\n",
    "data_path = '.'\n",
    "download_tinyImg200(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Training script structure and vgg-like network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обучить нейронную сеть, надо решить 5 задач:\n",
    "1. data loader (data provider) - как загружать и дополнять данные для обучения nn\n",
    "2. neural network architecture - что будет обучаться\n",
    "3. loss function (+ auxilary metrics on train and validation set) - как проверить качество нейронной сети\n",
    "4. optiimzer and training schedule - как будет обучаться нейронная сеть\n",
    "5. \"Train loop\" - что именно нужно делать для каждого пакета, как часто проверять ошибку проверки, как часто сохранять сеть и т.д. Этот код можно было бы написать в общем виде и повторно использовать в разных сценариях обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "g2i37mixtk9kkxkki1y8",
    "id": "rS_-00tYDMoB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our main computing device is 'cuda:0'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "def get_computing_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_computing_device()\n",
    "print(f\"Our main computing device is '{device}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data loader and data augmentation\n",
    "Обычно для манипулирования данными используются две связанные абстракции:\n",
    "- Dataset (`torch.utils.data.Dataset` и его подклассы из `torchvision.datasets\") - некоторый черный ящик, который хранит и предварительно обрабатывает отдельные элементы dataset. В частности, на этом уровне обычно находятся дополнения для отдельных образцов.\n",
    "- DataLoader (`torch.utils.data.DataLoader`) - структура, объединяющая отдельные элементы в пакетном режиме.\n",
    "\n",
    "Давайте разберемся с обучающим набором данных. Вот несколько простых дополнений, которые мы собираемся использовать в наших экспериментах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trainsforms = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.RandomRotation(5),\n",
    "     transforms.ColorJitter(0.5, 0.5, 0.5),\n",
    "     transforms.RandomGrayscale(),\n",
    "     # YOUR CODE : examine torchvision.transforms package, find transformation for color jittering\n",
    "     # and add it with proper parameters.\n",
    "     # transforms.SOME_OTHER_AUGMENTATION_FOR_COLOR_JITTER\n",
    "     # you may add any other transforms here\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для набора обучающих данных мы будем использовать пользовательский набор данных, который будет хранить все обучающие данные в оперативной памяти. Если у вас недостаточно оперативной памяти, вы можете использовать `torch vision.datasets.ImageFolder()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "jrzsbgniodgtg1hif324k9",
    "id": "5vq5Cm0ADMoK"
   },
   "outputs": [],
   "source": [
    "import tiny_img_dataset\n",
    "# you may use torchvision.datasets.ImageFolder() with the same parameters for loading train dataset \n",
    "train_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=train_trainsforms)\n",
    "# train_dataset = tiny_img_dataset.TinyImagenetRAM('tiny-imagenet-200/train', transform=train_trainsforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверка. Взгляните на папку `tiny-imagenet-200/val` и сравните ее с папкой `tiny-imagenet-200/train`. Выглядит по-другому, не так ли? Таким образом, мы не можем использовать `TinyImagenetRAM` для загрузки набора данных для проверки. Давайте вместо этого напишем пользовательский набор данных, но с таким же поведением, как у `TinyImagenetRAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class TinyImagenetValDataset(Dataset):\n",
    "    def __init__(self, root, transform=transforms.ToTensor()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        with open(os.path.join(root, 'val_annotations.txt')) as f:\n",
    "            annotations = []\n",
    "            for line in f:\n",
    "                img_name, class_label = line.split('\\t')[:2]\n",
    "                annotations.append((img_name, class_label))\n",
    "\n",
    "        # 1. define self.classes - list of sorted class labels from annotations\n",
    "        # it should look like self.classes from \"TinyImagenetRAM\"\n",
    "        # YOUR CODE\n",
    "        self.classes = sorted(list(set([label for _, label in annotations])))\n",
    "        \n",
    "        assert len(self.classes) == 200, len(self.classes)\n",
    "        assert all(self.classes[i] < self.classes[i+1] for i in range(len(self.classes)-1)), 'classes should be ordered'\n",
    "        assert all(isinstance(elem, type(annotations[0][1])) for elem in self.classes), 'your just need to reuse class_labels'\n",
    "\n",
    "        # 2. self.class_to_idx - dict from class label to class index\n",
    "        self.class_to_idx = {item: index for index, item in enumerate(self.classes)}\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images, self.targets = [], []\n",
    "        for img_name, class_name in tqdm.tqdm(annotations, desc=root):\n",
    "            img_name = os.path.join(root, 'images', img_name)\n",
    "            # 3. load image and store it in self.images (your may want to use tiny_img_dataset.read_rgb_image)\n",
    "            # store the class index in self.targets\n",
    "            # YOUR CODE\n",
    "            image = self.read_rgb_image(img_name)\n",
    "            \n",
    "            assert image.shape == (64, 64, 3), image.shape\n",
    "            self.images.append(Image.fromarray(image))\n",
    "            self.targets.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # take image and its target label from \"self.images\" and \"self.targets\", \n",
    "        # transform the image using self.transform and return the transformed image and its target label\n",
    "        \n",
    "        # YOUR CODE\n",
    "        image = self.images[index]\n",
    "        image = self.transform(image)\n",
    "        target = self.targets[index]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    @staticmethod\n",
    "    def read_rgb_image(path_to_image):\n",
    "        image = cv2.imread(path_to_image)\n",
    "        return  cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # np.array(img.convert(\"RGB\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте, наконец, загрузим набор данных для проверки. Обычно вам не применять аугментации для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-imagenet-200\\val:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-imagenet-200\\val: 100%|██████████| 10000/10000 [01:43<00:00, 96.60it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TinyImagenetValDataset('tiny-imagenet-200\\\\val', transform=transforms.ToTensor())\n",
    "\n",
    "assert all(train_dataset.classes[i] == val_dataset.classes[i] for i in range(200)), \\\n",
    "    'class order in train and val datasets should be the same'\n",
    "assert all(train_dataset.class_to_idx[elem] == val_dataset.class_to_idx[elem] for elem in train_dataset.classes), \\\n",
    "    'class indices should be the same'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для большинства случаев будет достаточно `DataLoader` по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellId": "6md8io0fesfby4r9per3jb",
    "id": "tY6OUeOODMoN"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellId": "hsq566ut87vokpkiq68",
    "id": "HBgW-gzwDMoQ"
   },
   "outputs": [],
   "source": [
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fxzxgbl11g2dixss4t9nx",
    "id": "arxSyhBLDMoX"
   },
   "source": [
    "### 1.2 Определение нейронной сети\n",
    "\n",
    "\"Сеть, подобная VGG\", обычно означает, что сеть представляет собой последовательность сверток с MaxPooling для понижающей размерности. Вот таблица из оригинальной статьи [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"].(https://arxiv.org/abs/1409.1556), который описывает классические конфигурации сетей VGG (часто называемые VGG-A, VGG-B и т.д. С использованием имени столбца в качестве идентификатора или VGG16, VGG19 и т.д. с использованием количества слоев в качестве идентификатора).\n",
    "\n",
    "![image.png](https://pytorch.org/assets/images/vgg.png)\n",
    "\n",
    "Эти сетевые конфигурации были разработаны для набора данных ImageNet. Поскольку изображения в tiny-imagenet имеют пониженный размер в 4 раза, мы собираемся разработать нашу собственную конфигурацию, уменьшив: \n",
    "1) количество слоев;\n",
    "2) количество нейронов в слоях;\n",
    "3) количество слоев с максимальным объединением, которые уменьшают выборку карт объектов\n",
    "\n",
    "Конфигурация нашей сети будет выглядеть следующим образом [Conv(16), Conv(16), MaxPool] + [Conv(32), Conv(32), MaxPool] + [Conv(64), Conv(64), MaxPool] + [Conv(128), Conv(128)] + [GlobalAveragePooling] + [FC(200) + softmax]\n",
    "\n",
    "\n",
    "Мы используем Conv(128) и GlobalAveragePooling вместо image flattening и слоев FC для уменьшения количества параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellId": "g5yf9z66xdpvq688ze2d8",
    "id": "7QF2hMVxDMoY"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6yn15hpuolcmryork2oqs",
    "id": "DJ6QKG3hDMoa"
   },
   "source": [
    "И еще кое-что. VGG был разработан до того, как был придуман BatchNormalization. В настоящее время было бы глупо, если бы мы не использовали BatchNormalization в нашей сети. Итак, давайте определим простой модуль, содержащий свертку, BatchNormalization и relu, и построим нашу сеть, используя этот модуль. Вот также реализация GlobalAveragePooling, приведенная для вас в качестве примера пользовательского модуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "cellId": "f985tf2dvssqwmyc6w99d",
    "id": "u_mbfRXMDMob"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "class GlobalAveragePool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=self.dim)\n",
    "\n",
    "    \n",
    "class ConvBNRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='same'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE: define vars for convolution, batchnorm, relu\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE: sequentially apply convolution, batchnorm, relu to 'x'\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def create_vgg_like_network(config=None):\n",
    "    \"\"\"\n",
    "    Creates VGG like network according to config\n",
    "    \"\"\"\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    default_config = [[16,16], [32, 32], [64, 64], [128, 128]]\n",
    "    config = config or default_config\n",
    "    \n",
    "    in_channels = 3\n",
    "    for block_index in range(len(config)):\n",
    "        for layer_index_in_block in range(len(config[block_index])):\n",
    "            out_channels = config[block_index][layer_index_in_block]\n",
    "            \n",
    "            # YOUR CODE: add ConvBNRelu module to model\n",
    "            model.add_module(f\"conv_{block_index}_{layer_index_in_block}\", ConvBNRelu(in_channels, out_channels, 3))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            \n",
    "        if block_index != len(config) - 1:\n",
    "            model.add_module(f'mp_{block_index}', nn.MaxPool2d(3, stride=2))\n",
    "            \n",
    "    model.add_module('pool', GlobalAveragePool(dim=(2,3)))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 200))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот и создана наша модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vgg_like_network()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7dh3d8xmkeinv4kx0g079",
    "id": "DvugZZbeDMoe"
   },
   "source": [
    "### 1.3 Определение функции потерь\n",
    "\n",
    "Обычно в качестве функции потерь для классификации изображений используется cross-entropy (отрицательное логарифмическое правдоподобие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cellId": "3y7p7o6s7vecpf3kpktj8v",
    "id": "cGEhRWMYDMof"
   },
   "outputs": [],
   "source": [
    "def compute_loss(predictions, gt):\n",
    "    return F.cross_entropy(predictions, gt).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Optimizer and training schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте обучим нашу сеть, используя Adam с параметрами по умолчанию. \n",
    "\n",
    "Для обучения с помощью `torch.optim.SGD` вам обычно нужно определить training schedule - способ, как снизить learning rate во время тренировки. Но поскольку в adam все градиенты масштабируются по моменту, эффект от правильного графика тренировок не так важен для обучения, как в SGD. Поэтому мы будем действовать как ленивые специалисты по обработке данных и не будем использовать шедулер вообще. Но вы можете поиграть с шедулером, используя, например, `torch.optim.lr_scheduler.ExponentialLR`, смотрите [документацию](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) с объяснением, как это использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Цикл обучения\n",
    "\n",
    "Давайте объединим ранее определенные элементы вместе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellId": "w8rht9ygh7uns89ypozln",
    "id": "sEy0LiHxDMol",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def eval_model(model, data_generator):\n",
    "    accuracy = []\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_generator:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "            \n",
    "def train_model(model, optimizer, train_data_generator):\n",
    "    train_loss = []\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (X_batch, y_batch) in tqdm.tqdm(train_data_generator):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # YOUR CODE: move X_batch, y_batch to 'device', compute model outputs on X_batch, \n",
    "        # run `compute_loss()` function\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        predictions = model(X_batch)\n",
    "        loss = compute_loss(predictions, y_batch)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # metrics\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def train_loop(model, optimizer, train_data_generator, val_data_generator, num_epochs):\n",
    "    \"\"\"\n",
    "    num_epochs - total amount of full passes over training data\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_model(model, optimizer, train_data_generator)\n",
    "        \n",
    "        val_accuracy = eval_model(model, val_data_generator)\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Обучение\n",
    "\n",
    "Вся подготовка завершена, пора запускать обучение!\n",
    "\n",
    "Обычно после обучения в течение 30 эпох вы должны получить нейронную сеть, которая предсказывает метки с точностью более 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 47\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, optimizer, train_data_generator, val_data_generator, num_epochs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     45\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 47\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m eval_model(model, val_data_generator)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Then we print the results for this epoch:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[49], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, train_data_generator)\u001b[0m\n\u001b[0;32m     18\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# enable dropout / batch_norm training behavior\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (X_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_data_generator):\n\u001b[0;32m     21\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# YOUR CODE: move X_batch, y_batch to 'device', compute model outputs on X_batch, \u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# run `compute_loss()` function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Jrytoeku Qtuhtc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Say Hello to ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части вам нужно переопределить вашу модель, все остальное останется прежним. Как и в случае с VGG, мы собираемся определить модель, подобную ResNet, а не классическую архитектуру, разработанную для классификации ImageNet.\n",
    "\n",
    "\"ResNet-подобный\" обычно означает, что ваша сеть состоит из \"residual блоков\". Существует два широко используемых типа блоков: с двумя и с тремя свертками:\n",
    "![resnet_blocks](https://miro.medium.com/max/613/1*zS2ChIMwAqC5DQbL5yD9iQ.png)\n",
    "\n",
    "На практике часто используются блоки с тремя свертками, поскольку они позволяют построить более глубокую сеть с меньшими параметрами. Блоки с двумя свертками обычно используются для сравнения с non-residual сетями, особенно с VGG и AlexNet.\n",
    "\n",
    "Вот таблица из статьи \"[Deep Residual Learning for Image Recognition]\"(https://arxiv.org/pdf/1512.03385.pdf), в которой описываются классические конфигурации сетей ResNet. Обычно их называют ResNet-18, ResNet-34 и так далее, используя количество слоев в качестве идентификатора. Обратите внимание, что сети, начиная с ResNet-50, основаны на 3-сверточных блоках. На самом деле ResNet-18 и ResNet-34 были представлены только для сравнения с VGG, в то время как ResNet-50 обычно используется на практике в качестве хорошего бейслайна.\n",
    "\n",
    "![изображение](https://miro.medium.com/max/2400/1*aq0q7gCvuNUqnMHh4cpnIw.png)\n",
    "\n",
    "Как и в случае с VGG, мы собираемся создать нашу собственную конфигурацию для сети. Давайте используем 2-сверточных блока для сравнения с vgg и возьмем сеть типа [Conv7x7 - 32] + [conv32-block, conv32-block] + [conv64-block, conv64-block] + [conv128-block, conv128-block] + [GlobalAveragePooling] + fc200 + softmax\n",
    "\n",
    "По сравнению с ResNet18, мы уменьшили количество фильтров и убрали max-pooling в начале и в последнем наборе сверток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock2(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implements the following function:\n",
    "    \n",
    "    output = relu(F(input) + Residual(input)), where: \n",
    "        Residual(x) = Conv + bn + relu + conv + bn\n",
    "        F(x) = x                                        , if in_channels == out_channels and stride == 1\n",
    "             = Conv1x1(in_channel, out_channel, stride) , otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
    "        super().__init__()\n",
    "        # YOUR CODE: define conv1, bn1, relu1, conv2, bn2 for residual branch computation \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = None  # conv for main branch adopatation\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, 1, stride, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # YOUR CODE: compute residual branch, \n",
    "        # DON'T OVERRIDE 'x' as you will need it \n",
    "        \n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(x)\n",
    "        residual = self.relu1(x)\n",
    "        residual = self.conv2(x)\n",
    "        residual = self.bn2(x)\n",
    "        \n",
    "        if self.conv3 is not None:\n",
    "            x = self.conv3(x)\n",
    "            \n",
    "        result = self.relu2(residual + x)\n",
    "        return result\n",
    "\n",
    "def create_resnet_like_network():\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    config = [[32, 32], [64, 64], [128, 128]]\n",
    "    model.add_module('init_conv', ConvBNRelu(3, 32, kernel_size=7, stride=2, padding=3))\n",
    "    \n",
    "    in_channels = 32\n",
    "    for i in range(len(config)):\n",
    "        for j in range(len(config[i])):\n",
    "            out_channels = config[i][j]\n",
    "            stride = 2 if i != 0 and j == 0 else 1\n",
    "            # YOUR CODE: add ResNetBlock2 module to model\n",
    "            model.add_module(f\"resnet_{i}_{j}\", ResNetBlock2(in_channels, out_channels))\n",
    "            \n",
    "            in_channels = out_channels\n",
    "    model.add_module('pool', GlobalAveragePool((2,3)))\n",
    "    model.add_module('logits', nn.Linear(out_channels, 200))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда давайте потренируем нашу сеть. Обычно после обучения в течение 30 эпох вы должны получить нейронную сеть, которая предсказывает метки с точностью >40% и дает около +1% профита по сравнению с vgg, из предыдущего эксперимента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# YOUR CODE: create resnet model, move it to 'device', create same optimizer as in previous experiment\n",
    "model = create_resnet_like_network().to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы внимательно изучали нашу сеть resnet, то могли заметить, что она имеет почти в 2 раза больше параметров и в 2 раза глубже, чем vgg. Давайте определим сеть, сопоставимую vgg, удвоив количество уровней conv.\n",
    "\n",
    "Наш новый сайт VGG-как архитектура будет [Сопу(16), усл. (16), MaxPool] + [Сопу(32), П(32), П(32), П(32), MaxPool] + [Сопу(64) отн(64) отн(64) отн(64), MaxPool] + [Сопу(128), Сопу(128), Сопу(128), Сопу(128)] + [GlobalAveragePooling] + [ФК(200) + softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_vgg_like_network(config=[[16,16], [32,32,32,32], [64, 64, 64, 64], [128, 128, 128, 128]])\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "train_loop(model, opt, train_batch_gen, val_batch_gen, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видите ли вы выгоду от residual связей? \n",
    "\n",
    "Качество сети vgg в этом эксперименте может быть даже хуже, чем качество сети vgg в первом эксперименте. Это связано с проблемой затухания градиента, которая затрудняет обучение глубоких нейронных сетей без residual связей."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seminar_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "notebookId": "0bd81ca7-4175-4905-a84c-21ed8da72299"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
